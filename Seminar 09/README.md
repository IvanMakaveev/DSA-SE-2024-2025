# Семинар №9 - Хеш таблици

## Преговор - Set, Map, Multiset, Multimap

## Въведение в хеш таблици
Ще разгледаме следният проблем - имаме някаква колекция от елементи, които искаме да бъдат адресирани. Представете си нещо подобно на вектор, в който имаме елементи от тип Т и ключове за адресация - индексите на вектора. Какво става, когато искаме да ползваме произволни такива ключове за, нека кажем, 10 елемента? Можем да създадем функция, която е биекция между тези 10 ключа и първите 10 индекса на вектора (0, 1, ... 9) и така ще си решим проблема - ще имаме колеция, която можем да адресираме с помощта на тези 10 ключа. Този подход работи добре, когато множеството от възможни ключове е "малко". Какво ще стане обаче, ако това мноежство е от 1000, 5000, дори безкраен брой елементи? 

### Какво е хеш функция
Хеш функция наричаме функция, която може да приема произволно голям вход от дадено (безкрайно) множество и връща като резултат стойност с фиксиран размер (от някакво крайно множество). Тази стойност наричаме хеш код на получените входни данни. 

Нека например нашата функция приема прозиволни string-ове и връща числа в диапазон от 0 до 500. Доста лесно можем да заключим, че по този начин можем да използваме string-ове като ключове, с които да адресираме някакви данни (най-много 500 такива) - примерно колекция от оценки, за която ключът е името на студента:

```c++ 
grades["Pesho"] = 5.50 // Could be expressed like grades[hash("Pesho")] = 5.50, where grades is a vector
grades["Gosho"] = 4.75
```

Разбира се, има множество проблеми, за които веднага можем да се сетим. Първо, за да бъде смислено цялото това преобразуване, функцията трябва да бъде детерминистична. Какво означава това? Ами, най-просто казано - за еднакъв вход, тя трябва да връща еднакъв изход (x == y => hash(x) == hash(y)).

Друг проблем би бил очевиден от следния пример: Нека дефинираме hash(str) = 0. Ако опитаме да работим с `grades["Pesho"]` и `grades["Gosho"]`, данните, които тези два ключа адресират, ще бъдат едни и същи - тези на индекс 0 в колекцията.

Следователно, свойство което бихме искали от хеш функцията е за x != y => hash(x) != hash(y) (Функцията да бъде инекция). Но за жалост, това изискване е невъзможно да бъде изискано. Защо? Ами, поради простото съображение, че домейнът на функцията е безкрайно множество, а кодомейнът - крайно. Следователно няма как да се конструира такава инекция. Все пак, ще искаме за максимално много стойности x != y това твърдение да е изпълнено.

(**Side note:** доъплнително е добре хеширащата функция да "разпръсква" данните в целия интервал. Тоест, ако x и y са някакви близки стойности, то hash(x) и hash(y) да не са непременно близки)

От фактът, че функцията няма как да бъде инективна, ясно се вижда, че тя не е обратима (няма как от hash да получим еднозначно оригиналният отговор).

Последното изискване, което ще поставим върху нашата хешираща функция е тя да работи "бързо", за да не променя асимптотиката на колекциите, които използват хеширане.

### Хеш таблица
Хеш таблицата е вид колекция, която позволява адресация на данни (от тип **Т**) с помощта на избран от потребителя тип на ключа (тип **К**).

Колекцията разполага с конкретен капацитет **N**, както и с хешираща функция **hash(K)**, която връща стойности в интервала [0, N). 

Повечето от проблемите на тази идея и техните решения разгледахме в дефиницията на хеш функция. Остана единствено да решим следният казус - какво става, когато два ключа имат еднакъв хеш? 

Събитието два различни ключа да имат еднакъв хеш код се нарича "колизия" (от английски - сблъскване). За да се справим по начало с него, освен функция за хеширане, ще ни трябва и функция за различаване на двата ключа (функция за сравнение). Който е писал на Java знае, че предефинирането на функцията **hashCode** върви винаги ръка за ръка с функцията **equals**. 

## Справяне с колизии
Нека разгледаме кои са най-честите стратегии за справяне с колизии:

### Separete chaining
Идеята тук е да пазим свързан списък с елементите с еднакъв хеш код. Нашата основна колекция е от **Node*** елементи, които играят роля на head указател за всеки свъразн списък. 

Когато трябва да се добави нов елемент към колекция, той се добавя в края на свързаният списък, намиращ се на получената от хеширането на ключа позиция.

Търсенето на елемент се случва в две фази - хешираме ключа (чрез **hashCode** функцията), за да видим в кой свързан списък да го търсим, и след това обхождаме свързания списък, за да го намерим (чрез **equals** функцията).

Премахването ще бъде сходно на търсенето, но с добавената стъпка да се пренасочат указателите и да се изтрие от паметта клетката, ако я има.

Каква е сложността на всяка от тези операции? Ами, зависи от това колко големи ще позволим да бъдат свързаните списъци. За да я минимизираме максимално, ще поискаме отношението между броя клетки в основната колекция (броя свързани списъци) и броя клетки сумарно измежду всички свързани списъци да бъде в някакво константно отношение (по-точно - в някакъв диапазон между две константи). След напускането на този диапазон, колекцията се преуразмерява и всички клетки се хешират и попълват наново.

Проблемът на тази имплементация е, че използваме свързани списъци и цялата идея да адресираме паметта чрез тези ключове леко се губи, тъй като силата на подобен вид колеции е именно, че разчитаме на непрекъснат блок памет, който може да се кешира (**locallity**). В случая това свойство се губи.

### Linear probing
Тук идеята е да работим със стандартна колекция (например вектор). При получаване на стойност i от хеширането на ключа опитваме да поставим елемента на тази позиция. Ако тя е заета - опитваме на позиция 2*i. Ако и тя е заета, продължаваме с 3i, 4i..., докато не намерим свободна. 

Проблем с тази имплементация е познат като клъстериране (**clustering**). Колкото повече елементи се добавят, толкова повече се формират "групи" от елементи в конкретни диапазони от масива, което впоследствие забавя операциите по добавяне, тъй като трябва да се прескочи целият клъстер, за да се достигне до следваща свободна позиция. Това пък от своя страна уголемява размера на клъстера и по този начин допълнително се забавят и последващите добавяния.

За справянето с този проблем отново се разчита на запазване на константо (в диапазон) отношение на броя заделени клетки и броя използвани такива. 

Друга трудност тук би била изтриването, което не може просто да остави свободна празнина в масива, тъй като тогава няма как да достигнем до елементи, които са били на позиция 2i, 3i и така нататък (ако позицията на триене е с хеш i). За целта ще маркираме тези елементи, като "изтрити", вместо, като "празни".

Още една често срещана оптимизация е използването на двойно хеширане. По този начин се намалява вероятността за колизия и се повишава скоростта на операциите.

## STL имплементации
```c++
std::unordered_map<K, V> hashMap;
std::unordered_set<V> hashSet;
```
## Решаване на задачи
[1. Маската на зоро](https://www.hackerrank.com/contests/sda-hw-7-2022/challenges/zoros-mask)

За домашно - по-сложна задача: [Cheater - 1D Battle boats](https://www.hackerrank.com/contests/sda-hw-7-2022/challenges/challenge-2590)

**Note:** В тази задача ще трябва да се използва std::set, тъй като ни интересува наредбата на изстрелите!

[2. Клюкарстване](https://www.hackerrank.com/contests/sda-hw-9-2023/challenges/challenge-4437/problem)

[3. Isomorphic strings](https://leetcode.com/problems/isomorphic-strings/description/)

[4. Group Anagrams](https://leetcode.com/problems/group-anagrams/description/)

[5. Subarrays Sum K](https://leetcode.com/problems/subarray-sum-equals-k/)

[6. 0-1 Subarray](https://www.hackerrank.com/contests/sda-homework-9/challenges/0-1-1/)

[7. Longest Consecutive Sequence](https://leetcode.com/problems/longest-consecutive-sequence/description/)

[8. Special Triples](https://www.hackerrank.com/contests/sda-hw-9-2022/challenges/challenge-3771/)

[9. DNA Sequence](https://leetcode.com/problems/repeated-dna-sequences/)